# -*- coding: utf-8 -*-
"""gpt.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ajVFRYVi9mUw7kmQwdYYRQLvD3Zhb2Hp
"""

!pip install torch

import torch
import torch.nn as nn
import torch.nn.functional as F

print("PyTorch version:", torch.__version__)
print("GPU available:", torch.cuda.is_available())

# Put some training text here
text = """
Artificial intelligence is a field of computer science that focuses on building systems
capable of performing tasks that normally require human intelligence. These tasks include
learning, reasoning, problem solving, understanding language, and recognizing patterns.

Machine learning is a subset of artificial intelligence. In machine learning, computers
learn from data instead of being explicitly programmed. The more data a system sees, the
better it can learn patterns and make predictions. Neural networks are one of the most
important tools used in machine learning.

A neural network is inspired by the human brain. It consists of layers of connected nodes
called neurons. Each neuron processes input data and passes the result to the next layer.
Deep learning refers to neural networks that have many layers. These deep networks are
especially powerful for tasks such as image recognition, speech recognition, and language
modeling.

Transformers are a special type of neural network architecture. Unlike older models,
transformers process data in parallel rather than sequentially. This makes them much faster
and more efficient. Transformers use a mechanism called attention to decide which parts of
the input are most important.

Self-attention allows a model to look at all words in a sentence at the same time and
understand how they relate to each other. This is especially useful for understanding long
sentences and complex language structures. Because of self-attention, transformers can
capture context more effectively than traditional models.

Large language models such as GPT are based on transformer architectures. These models are
trained on massive amounts of text data. During training, the model learns to predict the
next word or character based on the previous ones. Over time, the model learns grammar,
spelling, and even some level of reasoning.

Although large language models can generate impressive text, they do not truly understand
the world. They work by recognizing patterns in data. Despite this limitation, they are
extremely useful in applications such as chatbots, writing assistants, translation systems,
and educational tools.

Building a transformer model from scratch is challenging. It requires understanding linear
algebra, probability, and optimization. However, implementing a small version helps students
understand how modern AI systems work internally. Even a simple model demonstrates the core
ideas behind todayâ€™s most powerful AI technologies.

Learning by building is one of the best ways to understand artificial intelligence. By
training a model, observing its loss, and generating text, students gain hands-on experience
with machine learning concepts. This foundation is valuable for future work in data science,
research, and software engineering.
"""
print("Text length:", len(text))

print(text[:100])

# Create character-level vocabulary
chars = sorted(list(set(text)))
vocab_size = len(chars)

# character to integer mapping
stoi = { ch:i for i,ch in enumerate(chars) }
itos = { i:ch for i,ch in enumerate(chars) }

# encode and decode functions
encode = lambda s: [stoi[c] for c in s]
decode = lambda l: ''.join([itos[i] for i in l])

print("Vocabulary size:", vocab_size)
print("First 10 characters:", chars[:10])

# Test encode/decode
test_string = "Transformers are powerful"
encoded = encode(test_string)
decoded = decode(encoded)

print("Original:", test_string)
print("Encoded:", encoded[:10], "...")
print("Decoded:", decoded)

# Convert entire text to numbers
data = torch.tensor(encode(text), dtype=torch.long)

# Train / validation split
n = int(0.9 * len(data))
train_data = data[:n]
val_data = data[n:]

# Hyperparameters for batching
batch_size = 32
block_size = 64

def get_batch(split):
    data_split = train_data if split == 'train' else val_data
    ix = torch.randint(len(data_split) - block_size, (batch_size,))
    x = torch.stack([data_split[i:i+block_size] for i in ix])
    y = torch.stack([data_split[i+1:i+block_size+1] for i in ix])
    return x, y

# Test batch
xb, yb = get_batch('train')
print("x shape:", xb.shape)
print("y shape:", yb.shape)

# Self-Attention Head
class Head(nn.Module):
    def __init__(self, head_size):
        super().__init__()
        self.key = nn.Linear(n_embd, head_size, bias=False)
        self.query = nn.Linear(n_embd, head_size, bias=False)
        self.value = nn.Linear(n_embd, head_size, bias=False)

        # causal mask (prevents looking into the future)
        self.register_buffer("tril", torch.tril(torch.ones(block_size, block_size)))

    def forward(self, x):
        B, T, C = x.shape

        k = self.key(x)      # (B, T, head_size)
        q = self.query(x)    # (B, T, head_size)

        # attention scores
        wei = q @ k.transpose(-2, -1) / (C ** 0.5)
        wei = wei.masked_fill(self.tril[:T, :T] == 0, float("-inf"))
        wei = F.softmax(wei, dim=-1)

        v = self.value(x)
        out = wei @ v        # (B, T, head_size)
        return out

# Test self-attention head
n_embd = 128
head_size = 32

head = Head(head_size)
x = torch.randn(32, 64, n_embd)   # (batch, time, channels)

out = head(x)
print("Output shape:", out.shape)

# Multi-Head Attention
class MultiHeadAttention(nn.Module):
    def __init__(self, num_heads, head_size):
        super().__init__()
        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])
        self.proj = nn.Linear(num_heads * head_size, n_embd)

    def forward(self, x):
        out = torch.cat([h(x) for h in self.heads], dim=-1)
        return self.proj(out)

# Feed Forward Network
class FeedForward(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(n_embd, 4 * n_embd),
            nn.ReLU(),
            nn.Linear(4 * n_embd, n_embd),
        )

    def forward(self, x):
        return self.net(x)

# Transformer Block
class Block(nn.Module):
    def __init__(self, n_head):
        super().__init__()
        head_size = n_embd // n_head
        self.sa = MultiHeadAttention(n_head, head_size)
        self.ffwd = FeedForward()
        self.ln1 = nn.LayerNorm(n_embd)
        self.ln2 = nn.LayerNorm(n_embd)

    def forward(self, x):
        x = x + self.sa(self.ln1(x))
        x = x + self.ffwd(self.ln2(x))
        return x

# Test Transformer Block
n_embd = 128
n_head = 4

block = Block(n_head)
x = torch.randn(32, 64, n_embd)

out = block(x)
print("Block output shape:", out.shape)

# Model hyperparameters
n_embd = 128
n_head = 4
n_layer = 4
dropout = 0.2

# GPT Model
class GPT(nn.Module):
    def __init__(self, vocab_size):
        super().__init__()

        # Token + positional embeddings
        self.token_embedding = nn.Embedding(vocab_size, n_embd)
        self.position_embedding = nn.Embedding(block_size, n_embd)

        # Transformer blocks
        self.blocks = nn.Sequential(
            *[Block(n_head) for _ in range(n_layer)]
        )

        self.ln_f = nn.LayerNorm(n_embd)
        self.lm_head = nn.Linear(n_embd, vocab_size)

    def forward(self, idx, targets=None):
        B, T = idx.shape

        # embeddings
        tok_emb = self.token_embedding(idx)                  # (B,T,C)
        pos_emb = self.position_embedding(torch.arange(T))  # (T,C)
        x = tok_emb + pos_emb

        # transformer
        x = self.blocks(x)
        x = self.ln_f(x)

        logits = self.lm_head(x)                             # (B,T,vocab)

        if targets is None:
            loss = None
        else:
            logits = logits.view(-1, vocab_size)
            targets = targets.view(-1)
            loss = F.cross_entropy(logits, targets)

        return logits, loss

    def generate(self, idx, max_new_tokens):
        for _ in range(max_new_tokens):
            idx_cond = idx[:, -block_size:]
            logits, _ = self(idx_cond)
            logits = logits[:, -1, :]
            probs = F.softmax(logits, dim=-1)
            next_idx = torch.multinomial(probs, num_samples=1)
            idx = torch.cat((idx, next_idx), dim=1)
        return idx

# Test GPT forward pass
model = GPT(vocab_size)
x, y = get_batch('train')

logits, loss = model(x, y)
print("Logits shape:", logits.shape)
print("Loss:", loss.item())

# Training setup
device = "cuda" if torch.cuda.is_available() else "cpu"
model = GPT(vocab_size).to(device)

optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)

# Move data to device
def get_batch_device(split):
    x, y = get_batch(split)
    return x.to(device), y.to(device)

# Training loop
max_iters = 3000
eval_interval = 300

for step in range(max_iters):
    xb, yb = get_batch_device('train')
    logits, loss = model(xb, yb)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if step % eval_interval == 0:
        print(f"Step {step} | Loss: {loss.item():.4f}")

# Generate sample text
context = torch.zeros((1, 1), dtype=torch.long).to(device)
generated = model.generate(context, max_new_tokens=300)
print(decode(generated[0].tolist()))