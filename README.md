# Bigram Language Model (Mid-Term Assignment)

## Overview
This project implements a character-level Bigram Language Model.  
The model predicts the next character based only on the current character, without any memory of previous context.

## Dataset
The model is trained on the Tiny Shakespeare dataset, which contains text from Shakespeareâ€™s plays.

## Methodology
- Unique characters are extracted from the dataset to form a vocabulary.
- A bigram count matrix is built to count how often one character follows another.
- The counts are normalized into probability distributions.
- Text is generated by sampling the next character based on these probabilities.

## Result
The generated output resembles broken or gibberish English, which is expected behavior for a bigram model due to the lack of long-term context.

A screenshot of the generated text output is included in this repository.

## Files
- `bigram_model.ipynb` : Google Colab notebook containing the full implementation
- `generated_text_output.png` : Screenshot of generated gibberish text

## Conclusion
This project demonstrates how a simple statistical language model can learn character transition probabilities and generate text without understanding meaning or context.
